{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OG6tf4aciwu"
      },
      "source": [
        "# **Introducción a la visión artificial con Python y OpenCV**\n",
        "\n",
        "\n",
        "<div style=\"display: flex;gap: 400px; align-items: center;\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\" alt=\"Logo de Python\" width=\"150\" style=\"margin-right: 400px;\">\n",
        "  <img src=\"https://www.cidaut.es/wp-content/uploads/2023/12/logotipo-simple-color.png\" alt=\"Logo de Cidaut\" width=\"220\">\n",
        "</div>\n",
        "\n",
        "\n",
        "En este taller vamos a aprender a ejecutar algunas tareas muy simples de visión artificial. Para simplificarlo más, el tutorial será realizado con Python. La idea es daros a conocer nuevas herramientas que se utilizan en este campo y que ya están desarrolladas. Las principales librerías que vamos a utilizar para esto son:\n",
        "- `OpenCV`. La librería más conocida de visión artificial en Python y C++. Es muy útil para construir nuevos proyectos o librerías en base a sus métodos. Utiliza la formulación tensorial de `Numpy`.\n",
        "\n",
        "- `Ultralytics`. Una librería moderna basada en `PyTorch` y `OpenCV`. Mantiene y mejora el algoritmo rápido de detección YOLO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqdahsWdeM90"
      },
      "source": [
        "## **Índice**\n",
        "\n",
        "Con este taller se pretende que obtengais recursos básicos para:\n",
        "\n",
        "- Leer una imagen con OpenCV y aplicar filtros de procesamiento tradicionales.\n",
        "- Repetir esto con vídeos.\n",
        "- Utilizar YOLO para detectar, segmentar y estudiar objetos de la imagen.\n",
        "\n",
        "Los contenidos están separados en:\n",
        "\n",
        "1. Procesamiento de imagen con OpenCV.\n",
        "2. Procesamiento de vídeo con OpenCV.\n",
        "3. Aplicación del algoritmo de detección YOLO.\n",
        "4. Trabajo Individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWDu6nuBTxYd"
      },
      "source": [
        "### # Descarga de scripts\n",
        "\n",
        "En primer lugar, vamos a descargar e introducir en nuestro entorno de trabajo scripts que más adelante utilizaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpkxefSEclld",
        "outputId": "b3fc816e-26fb-4398-da64-211bccbe4b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading png2ascii.py...\n",
            "--2025-10-28 10:53:52--  https://raw.githubusercontent.com/cidautai/Taller-CV/main/png2ascii.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4215 (4.1K) [text/plain]\n",
            "Saving to: ‘png2ascii.py’\n",
            "\n",
            "png2ascii.py        100%[===================>]   4.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-28 10:53:53 (67.8 MB/s) - ‘png2ascii.py’ saved [4215/4215]\n",
            "\n",
            "Downloading quantized.py...\n",
            "--2025-10-28 10:53:53--  https://raw.githubusercontent.com/cidautai/Taller-CV/main/quantized.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3447 (3.4K) [text/plain]\n",
            "Saving to: ‘quantized.py’\n",
            "\n",
            "quantized.py        100%[===================>]   3.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-28 10:53:53 (62.7 MB/s) - ‘quantized.py’ saved [3447/3447]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Lista de archivos a descargar\n",
        "archivos = ['png2ascii.py', 'quantized.py']\n",
        "\n",
        "for archivo in archivos:\n",
        "    if not os.path.exists(archivo):\n",
        "        print(f\"Downloading {archivo}...\")\n",
        "        !wget https://raw.githubusercontent.com/cidautai/Taller-CV/main/{archivo}\n",
        "    else:\n",
        "        print(f\"{archivo} ya existe.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9TJ2ZctfLxY"
      },
      "source": [
        "### **1. Procesamiento de imagen con OpenCV.**\n",
        "\n",
        " <img src=\"https://opencv.org/wp-content/uploads/2020/07/OpenCV_logo_black-2.png\" alt=\"Logo de OpenCV\" width=\"150\" style=\"margin-right: 400px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujL2DakNcSg7"
      },
      "outputs": [],
      "source": [
        "# Instalamos las librerías necesarias para este apartado\n",
        "!pip install opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9HBy3YNsB8r"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtnVnQx206L5"
      },
      "source": [
        "Definimos ahora una función con la que poder descargar imágenes de la web. Para ello utilizamos la librería `requests`.\n",
        "\n",
        "Representaremos las imágenes con la librería `matplotlib`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJR0NAu1shCC"
      },
      "outputs": [],
      "source": [
        "# URL de la imagen\n",
        "def get_img_from_url(url = \"https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\"):\n",
        "\n",
        "  response = requests.get(url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
        "  image_array = np.frombuffer(response.content, np.uint8)\n",
        "  image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "  # Pasar de BGR a RGB, ya que OpenCV usa BGR por defecto\n",
        "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image_rgb\n",
        "image_rgb = get_img_from_url()\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(image_rgb)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAAQte9PuKia"
      },
      "source": [
        "Las imágenes en OpenCV son arrays de numpy con dimensiones [H,W,C], donde H es el alto, W es el ancho y C el número de canales. Generalmente el número de canales es 3, y se sigue el formato RGB:\n",
        "\n",
        " <img src=\"https://raw.githubusercontent.com/brohrer/blog_images/refs/heads/main/image_processing/three_d_array.png\n",
        " \" alt=\"Representación RGB\" width=\"300\" style=\"margin-right: 400px;\">\n",
        "\n",
        "Con la función de OpenCV [cvtColor()](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html) podremos hacer las transformaciones de color más típicas en una imagen. Probemos a convertir esta imagen a escala de grises. Es en este espacio en el que calcularemos los bordes de la imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYtHm1tVtW6t"
      },
      "outputs": [],
      "source": [
        "gray = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "plt.imshow(gray, cmap= \"gray\")\n",
        "plt.title(\"Lenna - Grises\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSDmjgKZvFRT"
      },
      "source": [
        "Ya en este dominio, podemos detectar los bordes. Para este caso vamos a utilizar una de las primeras soluciones a este problema: los filtros de Sobel. Estos se pueden interpretar como una aproximación discreta a las derivadas direccionales. Típicamente existen dos tipos, el filtro en X y el filtro en Y, calculando cada uno la derivada parcial discreta a lo largo de esa dirección:\n",
        "\n",
        " <img src=\"https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/sobmasks.gif\" alt=\"Filtros de Sobel\" width=\"250\" style=\"margin-right: 400px;\">\n",
        "\n",
        "Los filtros se aplican a través de una convolución. La derivada a lo largo del eje x la podemos aproximar por Dx = Gx * Imagen, y lo mismo en el caso de y. * es la operación de convolución que se ilustra en el siguiente GIF:\n",
        "\n",
        " <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif\" alt=\"Convolución\" width=\"250\" style=\"margin-right: 400px;\">\n",
        "\n",
        "> Dado que openCV trabaja con tensores de `numpy` podemos utilizar las funciones definidas en esta librería para realizar transformaciones de imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHlJTacburGZ"
      },
      "outputs": [],
      "source": [
        "# Calculamos las derivadas en X e Y con los filtros de Sobel\n",
        "sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "# Calculamos la magnitud de el cambio por pixel usando la norma euclidea\n",
        "sobel_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
        "# Expresamos este valor en el rango [0, 255] para poder representarlo\n",
        "sobel_magnitude = np.uint8(255 * sobel_magnitude / np.max(sobel_magnitude))\n",
        "\n",
        "plt.imshow(sobel_magnitude, cmap='gray')\n",
        "plt.title(\"Bordes detectados\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L53aj8F6xOj9"
      },
      "source": [
        "Esta es una de las soluciones más sencillas que existen para resolver esta tarea. Si queréis probar otras opciones más potentes, podéis consultar el algoritmo `Canny` en el correspondiente blog de la página de [OpenCV](https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huiy5dmdQneB"
      },
      "source": [
        "Con esto ya vemos que existen herramientas para analizar imágenes. A continuación presentamos otras dos funciones que se pueden crear con imagen. Con esto queremos que tengáis una idea de como crear vuestros propios procesamientos de imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjfvVqdqQwA4"
      },
      "outputs": [],
      "source": [
        "# importamos las funciones necesarias de los scripts descargados\n",
        "import importlib, sys, os\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "from quantized import quick_quantize_video, quantize_image\n",
        "from png2ascii import image_to_ascii_art"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zu2MP0jR-Fp"
      },
      "source": [
        "Siguiendo con la imagen que teníamos antes o con otra nueva que os interese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDTbl86JRFmb"
      },
      "outputs": [],
      "source": [
        "image_rgb = get_img_from_url()\n",
        "\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(image_rgb)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa0L9rCHSxeF"
      },
      "source": [
        "### Quantización del color\n",
        "Primero comencemos cambiando los valores de intensidad que puede tomar la imagen. Hasta ahora leíamos las imágenes con valores para cada pixel en el rango [0,255]. Digamos que en lugar de tener 256 valores distintos, podemos tener solo 5.\n",
        "\n",
        "Ahora nos vamos a encontrar con muchos menos detalles, ya que hemos hecho una quantización bastante agresiva. Esta reducción del rango de valores se utiliza comúnmente en la grabación de vídeos o streaming, ya que permite **reducir** notablemente el peso en memoria del vídeo. Este y otros tipos de algoritmos se conocen como algoritmos de compresión de la imagen. Uno de los métodos más conocidos de compresión es el JPEG:\n",
        "\n",
        " <img src=\"https://comprimeme.wordpress.com/wp-content/uploads/2014/12/lenna-600x197.png\" alt=\"Compresión JPEG\" width=\"600\" style=\"margin-right: 400px;\">\n",
        "\n",
        "Podeis probar con varios valores de `k` o utilizar el método `kmeans` en vez de `uniform`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egxz2hjeUIGL"
      },
      "outputs": [],
      "source": [
        "quant_image = quantize_image(image_rgb, k=5, method= 'uniform')\n",
        "\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(quant_image)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9gjXr02ScOC"
      },
      "source": [
        "### Transformación a ASCII\n",
        "\n",
        "Ya que hemos visto que los valores de intensidad de la imagen se pueden mapear por otros valores, vamos a sustituirlos por algo completamente diferente: caracteres ASCII. El funcionamiento es muy semejante al de la quantización de color que hemos hecho antes. Asociamos a cada uno de los carácteres ASCII un rango de valores de intensidad y sustituímos en la imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOaKOWTmTVh4"
      },
      "outputs": [],
      "source": [
        "ascii_image= image_to_ascii_art(image_rgb, output_path = 'ascii_lena.png')\n",
        "\n",
        "print(ascii_image)\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(ascii_image)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97nXcgb-06ep"
      },
      "source": [
        "## **2. Procesar video**\n",
        "\n",
        "Una vez visto como se procesan imágenes, podemos plantearnos el procesado de vídeos. En este caso la forma que tendremos de hacerlo con OpenCV es ir recorriendo el vídeo frame a frame, y procesar cada uno de ellos de forma independiente.\n",
        "\n",
        "Escoged el vídeo que queráis para esta tarea. Lo más rápido puede ser cogerlo de Youtube. Coged un vídeo corto, tipo *short* para que los tiempos de procesamiento sean menores. Para descargar el vídeo utilizamos la librería `yt-dlp`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAe6909T1Vii"
      },
      "outputs": [],
      "source": [
        "# instalamos yt-dlp\n",
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiZDTvGJxMA_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import yt_dlp\n",
        "\n",
        "video_url = \"https://www.youtube.com/shorts/n96w_-EXieY\"\n",
        "output_path = \"short_video.mp4\"\n",
        "\n",
        "# Configurar opciones para descargar el mejor mp4 posible\n",
        "ydl_opts = {\n",
        "    'outtmpl': output_path,   # nombre del archivo de salida\n",
        "    'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',  # preferir mp4\n",
        "    'quiet': True\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])\n",
        "\n",
        "print(\"Video descargado:\", output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OdFWBhx25Ps"
      },
      "source": [
        "Una vez descargado ya podemos empezar a trabajar en él. Para ello nos valemos de la clase `VideoCapture()` para crear el objeto vídeo. De esta podemos extraer multitud de parámetros del mismo. Como queremos guardar el vídeo una vez procesado, recogemos los FPS y tamaño original original, para así preservar sus características. El objeto sobre el que guardamos el vídeo es de la clase `VideoWriter()`. En ambos objetos, al finalizalir el procesamiento deberemos aplicar su método *release()*.\n",
        "\n",
        "> La clase `VideoCapture()` es realmente útil para aplicaciones en tiempo real, ya que permite recibir distintas entradas de vídeo, como cámaras conectadas al equipo o *streaming*.\n",
        "\n",
        "Vamos a obtener los bordes de la imagen usando los filtros de Sobel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaQni2Je1rxA"
      },
      "outputs": [],
      "source": [
        "input_path = \"short_video.mp4\"\n",
        "output_path = \"video_sobel.mp4\"\n",
        "\n",
        "# Abrir el video original\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "# Obtener dimensiones y FPS del video de entrada\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear VideoWriter para el resultado (video en escala de grises)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height), False)\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir a escala de grises\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Aplicar Sobel en X e Y\n",
        "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "    # Calcular magnitud del gradiente y normalizar\n",
        "    sobel_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
        "    sobel_magnitude = np.uint8(255 * sobel_magnitude / np.max(sobel_magnitude))\n",
        "\n",
        "    # Escribir el frame procesado directamente en el nuevo video\n",
        "    out.write(sobel_magnitude)\n",
        "\n",
        "    frame_idx += 1\n",
        "    if frame_idx % 50 == 0:\n",
        "        print(f\"Procesados {frame_idx} frames...\")\n",
        "\n",
        "# Liberar recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"Video procesado guardado en: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyy3yTAnaze2"
      },
      "source": [
        "Y al igual que hicimos con la imagen, también podríamos quantizar el color del vídeo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azAaF59za5kn"
      },
      "outputs": [],
      "source": [
        "quant_video = quick_quantize_video('short_video.mp4', output_path='video_quant.mp4', colors = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5czI2YgjuyGj"
      },
      "source": [
        "Estas son algunas de las muchas cosas que se pueden plantear con la librería OpenCV. Si queréis formaros más en este tema, disponéis de unos [tutoriales](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) muy buenos en su página principal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzXoAJBe_VAF"
      },
      "source": [
        "## **3. Procesamiento con YOLO**\n",
        "\n",
        "\n",
        " <img src=\"https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/68e4eb1e9893320b26cc02c3_Ultralytics%20Logo.png.svg\" alt=\"Logo de OpenCV\" width=\"300\" style=\"margin-right: 400px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsjJUzaPF05g"
      },
      "source": [
        "En esta sección vamos a explorar una aplicación más avanzada de la visión artificial:\n",
        "la detección y/o reconocimiento de objetos mediante redes neuronales convolucionales.\n",
        "\n",
        "Para ello utilizaremos la librería `ultralytics`, que incluye los modelos YOLO\n",
        "(*You Only Look Once*). Estos modelos destacan por su rapidez y precisión,\n",
        "siendo ampliamente usados en detección de personas, vehículos o animales en tiempo real.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIz6q1dAGF3z"
      },
      "source": [
        "Instalamos la librería con el siguiente comando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9OdL4le4VrF"
      },
      "outputs": [],
      "source": [
        "# instalamos ultralytics\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhh9iio53-L2"
      },
      "outputs": [],
      "source": [
        "import ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeXnI4lbGLg-"
      },
      "source": [
        "Cargamos ahora un modelo preentrenado de YOLO.\n",
        "Si no se indica un modelo específico, por defecto se utiliza `yolov8s.pt`. Pero es una versión un tanto desactualizada.\n",
        "\n",
        "Por ello, en este caso usaremos `yolo11s.pt`, una versión optimizada y ligera, adecuada para demostraciones y ejecución rápida en CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wKkGAnGGYqS"
      },
      "source": [
        "Ejecutamos la detección con el modelo cargado. El método `model()` realiza todo el proceso:\n",
        "- Inferencia sobre la imagen.\n",
        "- Detección de objetos y sus coordenadas.\n",
        "- Asignación de etiquetas y confianza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_nHSBr94Fyl"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# Cargar un modelo pre-entrenado (por ejemplo, YOLOv8s)\n",
        "# Se usará yolov8s.pt por defecto si no se especifica otro\n",
        "model = YOLO('yolo11s.pt')\n",
        "\n",
        "# URL de la imagen para detección\n",
        "image_url = \"https://ultralytics.com/images/bus.jpg\"\n",
        "\n",
        "# Descargamos la imagen desde la URL dada\n",
        "image_rgb = get_img_from_url(url=image_url)\n",
        "\n",
        "\n",
        "# Ejecutar la detección en la imagen\n",
        "results = model(image_rgb)\n",
        "\n",
        "# Guardar la imagen con las detecciones en el directorio por defecto ('runs/detect/...')\n",
        "for r in results:\n",
        "    im_array = r.plot()  # Ploteo de imagen RGB en formato numpy de las predicciones del modelo ejecutado\n",
        "    im = cv2.cvtColor(im_array, cv2.COLOR_RGB2BGR) # Convertimos RGB a BGR\n",
        "    output_path = 'output_yolo.png'  # Elegimos un nombre para guardar la imagen\n",
        "    cv2.imwrite(output_path, im) # Finalmente guardamos la imagen BGR con OpenCV,\n",
        "\n",
        "\n",
        "print(f\"Imagen con detecciones guardada en: {output_path}\")\n",
        "\n",
        "img_to_plot = np.concatenate((image_rgb, im_array), axis=1)\n",
        "# Opcional: mostrar la imagen con detecciones (puede ser grande)\n",
        "plt.imshow(img_to_plot)\n",
        "# plt.title(\"YOLO Detecciones\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhsDlqO3GkQk"
      },
      "source": [
        "Podemos aplicar exactamente el mismo proceso a un vídeo. La librería `ultralytics`\n",
        "permite procesar directamente vídeos o flujos de cámara mediante el método `predict()`.\n",
        "\n",
        "El parámetro `save=True` guarda automáticamente el vídeo anotado en el directorio `runs/detect/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMxS5PJR3OoW"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Cargar un modelo pre-entrenado (por ejemplo, YOLOv8s)\n",
        "model = YOLO('yolo11s.pt')\n",
        "\n",
        "# Ruta del video descargado\n",
        "video_path = 'short_video.mp4'\n",
        "\n",
        "# Ejecutar la detección en el video\n",
        "# El método predict de YOLO puede procesar videos directamente\n",
        "results = model.predict(video_path, stream=True, save=True, project='.', name= 'deteccion_video', verbose=False, ) # save=True guarda el video con las detecciones\n",
        "\n",
        "print(\"Procesamiento del video con YOLO completado. El video con las detecciones se ha guardado en el directorio 'runs/detect'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbbKwr-_Gqaw"
      },
      "source": [
        "Ya para terminar, vamos a darle un uso a lo que hemos visto en el ejemplo anterior. Aprovechando la detección de personas, vamos a darle una aplicación muy sencilla y a la vez muy útil.\n",
        "\n",
        "En concreto, vamos a usar YOLO para contar personas en la región de un vídeo dentro de una cola de embarque en un aeropuerto.\n",
        "Este tipo de solución se usa en control de aforos o sistemas automáticos en aeropuertos y estaciones.\n",
        "\n",
        "Para ello utilizaremos el módulo `ObjectCounter` incluido en `ultralytics.solutions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ-R6-hejRIK"
      },
      "outputs": [],
      "source": [
        "from ultralytics.utils.downloads import safe_download\n",
        "from ultralytics import solutions\n",
        "\n",
        "# Descarga de un video de ejemplo\n",
        "safe_download(\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/queue-management-demo.mp4\")\n",
        "\n",
        "\n",
        "def count_objects_in_region(video, output_video_path, model_path, classes= [0]):\n",
        "    \"\"\"Cuenta objetos dentro de una region especifica del video.\"\"\"\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    assert cap.isOpened(), \"Error leyendo el archivo de video\"\n",
        "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "    # Region de interes definida por cuatro puntos\n",
        "    region_points = [(250, 570), (1200, 350), (1200, 390), (250, 610)] \n",
        "\n",
        "    # Inicializamos el contador de objetos\n",
        "    counter = solutions.ObjectCounter(show=True, show_out=False, verbose=False, region=region_points, model=model_path, classes = classes)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, im0 = cap.read()\n",
        "        if not success:\n",
        "            print(\"Video finalizado o frame vacio.\")\n",
        "            break\n",
        "        results = counter(im0)\n",
        "        video_writer.write(results.plot_im)\n",
        "\n",
        "    cap.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Ejecutamos la función de conteo sobre el video de ejemplo\n",
        "count_objects_in_region(\"queue-management-demo.mp4\", \"output_video_queue.mp4\", \"yolo11n.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si queréis conocer otro tipo de soluciones ya implementadas por `ultralytics` podéis consultar el siguiente [link](https://docs.ultralytics.com/es/solutions/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **4. Trabajo Individual**\n",
        "\n",
        "Ahora os toca a vosotros. En lo que resta de taller os proponemos hacer alguna aplicación que use los contenidos tratados. Alguna idea de trabajo podría ser:\n",
        "\n",
        "- En relación al procesamiento de imagen, extender el uso del transformador de imagen a ASCII a un vídeo.\n",
        "- Con respecto al uso del algoritmo YOLO, buscar otros vídeos en los que se pueda realizar el conteo de objetos en una región (p.ej. carreteras)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
