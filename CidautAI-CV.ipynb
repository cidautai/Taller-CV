{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introducción a la visión artificial con Python y OpenCV**\n",
        "\n",
        "\n",
        "<div style=\"display: flex;gap: 400px; align-items: center;\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg\" alt=\"Logo de Python\" width=\"150\" style=\"margin-right: 400px;\">\n",
        "  <img src=\"https://www.cidaut.es/wp-content/uploads/2023/12/logotipo-simple-color.png\" alt=\"Logo de Cidaut\" width=\"220\">\n",
        "</div>\n",
        "\n",
        "\n",
        "En este taller vamos a aprender a ejecutar algunas tareas muy simples de visión artificial. Para simplifircalo más el tutorial será realizado con Python. La idea es daros a conocer nuevas herramientas que se utilizan en este campo y que ya están desarrolladas. Las principales librerías que vamos a utilizar para esto son:\n",
        "- `OpenCV`. La librería más conocida de visión artificial en Python y C++. Es muy útil para construir nuevos proyectos o librerías en base a sus métodos. Utiliza la formulación tensorial de `Numpy`.\n",
        "\n",
        "- `Ultralytics`. Una libreria moderna basada en `PyTorch` y `OpenCV`. Mantiene y mejora el algoritmo rápido de detección YOLO."
      ],
      "metadata": {
        "id": "5OG6tf4aciwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Índice**\n",
        "\n",
        "Con este taller se pretende que obtengais recursos básicos para:\n",
        "\n",
        "- Leer una imagen con OpenCV y aplicar filtros de procesamiento tradicionales.\n",
        "- Repetir esto con videos.\n",
        "- Utilizar YOLO para detectar, segmentar y estudiar objetos de la imagen."
      ],
      "metadata": {
        "id": "mqdahsWdeM90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Descarga de scripts"
      ],
      "metadata": {
        "id": "SWDu6nuBTxYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cidautai/Taller-CV/blob/main/png2ascii.py\n",
        "!wget https://github.com/cidautai/Taller-CV/blob/main/quantized.py"
      ],
      "metadata": {
        "id": "GdqT2ChIT2HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Procesamiento de imagen con OpenCV.\n",
        "\n",
        " <img src=\"https://opencv.org/wp-content/uploads/2020/07/OpenCV_logo_black-2.png\" alt=\"Logo de OpenCV\" width=\"150\" style=\"margin-right: 400px;\">"
      ],
      "metadata": {
        "id": "k9TJ2ZctfLxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujL2DakNcSg7"
      },
      "outputs": [],
      "source": [
        "# Instalamos las librerías necesarias para este apartado\n",
        "!pip install opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests"
      ],
      "metadata": {
        "id": "e9HBy3YNsB8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos ahora una función con la que poder descargar imágenes de la web. Para ello utilizamos la librería `requests`.\n",
        "\n",
        "Representaremos las imágenes con la librería `matplotlib`."
      ],
      "metadata": {
        "id": "jtnVnQx206L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL de la imagen\n",
        "def get_img_from_url(url = \"https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png\"):\n",
        "\n",
        "  response = requests.get(url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
        "  image_array = np.frombuffer(response.content, np.uint8)\n",
        "  image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "  # Pasar de BGR a RGB, ya que OpenCV usa BGR por defecto\n",
        "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image_rgb\n",
        "image_rgb = get_img_from_url()\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(image_rgb)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AJR0NAu1shCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la función de OpenCV [cvtColor()](https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html) podremos hacer las transformaciones de color más típicas en una imagen. Probemos a convertir esta imagen a escala de grises. Es en este espacio en el que calcularemos los bordes de la imagen."
      ],
      "metadata": {
        "id": "CAAQte9PuKia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gray = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "plt.imshow(gray, cmap= \"gray\")\n",
        "plt.title(\"Lenna - Grises\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SYtHm1tVtW6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya en este dominio, podemos detectar los bordes. Para este caso vamos a utilizar una de las primeras soluciones a este problema: los filtros de Sobel. Estos se pueden interpretar como una aproximación discreta a las derivadas direccionales. Típicamente existen dos tipos, el filtro en X y el filtro en Y, calculando cada uno la derivada parcial discreta a lo largo de esa dirección:\n",
        "\n",
        " <img src=\"https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/sobmasks.gif\" alt=\"Filtros de Sobel\" width=\"250\" style=\"margin-right: 400px;\">\n",
        "\n",
        "Los filtros se aplican a través de una convolución. La derivada a lo largo del eje x la podemos aproximar por Dx = Gx * Imagen, y lo mismo en el caso de y. * es la operación de convolución que se ilustra muy fácil con el siguiente GIF:\n",
        "\n",
        " <img src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif\" alt=\"Convolución\" width=\"250\" style=\"margin-right: 400px;\">\n",
        "\n",
        "> Dado que openCV trabaja con tensores de `numpy` podemos utilizar las funciones definidas en esta librería para realizar transformaciones de imagen."
      ],
      "metadata": {
        "id": "xSDmjgKZvFRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos las derivadas en X e Y con los filtros de Sobel\n",
        "sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "# Calculamos la magnitud de el cambio por pixel usando la norma euclídea\n",
        "sobel_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
        "# Expresamos este valor en el rango [0, 255] para poder representarlo\n",
        "sobel_magnitude = np.uint8(255 * sobel_magnitude / np.max(sobel_magnitude))\n",
        "\n",
        "plt.imshow(sobel_magnitude, cmap='gray')\n",
        "plt.title(\"Bordes detectados\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WHlJTacburGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta es la solución más simple que existe a la obtención de los bordes. Si quereis probar otras opciones más potentes, podéis consultar el algoritmo `Canny` en el correspondiente blog de la página de [OpenCV](https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html)."
      ],
      "metadata": {
        "id": "L53aj8F6xOj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto ya vemos que existen herramientas para analizar imágenes. A continuación presentamos otras dos funciones que se pueden crear con imagen. Con esto queremos que tengáis una idea de como crear vuestros propios procesamientos de imagen."
      ],
      "metadata": {
        "id": "Huiy5dmdQneB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from quantized import quick_quantize_video, quantize_image\n",
        "from png2ascii import image_to_ascii_art"
      ],
      "metadata": {
        "id": "YjfvVqdqQwA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiendo con la imagen que teníamos antes, o con otra nueva que os interese."
      ],
      "metadata": {
        "id": "0Zu2MP0jR-Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_rgb = get_img_from_url()\n",
        "\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(image_rgb)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fDTbl86JRFmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantización del color\n",
        "Primero comencemos cambiando los valores de intensidad que puede tomar la imagen. Hasta ahora leiamos las imágenes con valores para cada pixel en el rango [0,255]. Digamos que en lugar de tener 256 valores distintos, podemos tener solo 5 valores distintos.\n",
        "\n",
        "Ahora nos vamos a encontrar con muchos menos detalles, ya que hemos hecho una quantización bastante agresiva. Esta reducción del rango de valores se utiliza comunmente en la grabación de vídeos o streaming, ya que permite **reducir** notablemente el peso en memoria del vídeo. Este y otros tipos de algoritmos se conocen como algoritmos de compresión de la imagen. Uno de los métodos más conocidos de compresión es el JPEG:\n",
        "\n",
        " <img src=\"https://comprimeme.wordpress.com/wp-content/uploads/2014/12/lenna-600x197.png\" alt=\"Filtros de Sobel\" width=\"600\" style=\"margin-right: 400px;\">\n",
        "\n",
        "Podeis probar con varios valores de `k` o utilizar el método `kmeans` en vez de `uniform`."
      ],
      "metadata": {
        "id": "fa0L9rCHSxeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_image = quantize_image(image_rgb, k=5, method= 'uniform')\n",
        "\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(quant_image)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Egxz2hjeUIGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformación a ASCII\n",
        "\n",
        "Ya que hemos visto que los valores de intensidad de la imagen se pueden mapear por otros valores, vamos a sustituirlos por algo completamente diferente: caracteres ASCII. El funcionamiento es muy semejante al de la quantización de color que hemos hecho antes. Asociamos a cada uno de los carácteres ASCII un rango de valores de intensidad y sustituimos en la imagen."
      ],
      "metadata": {
        "id": "y9gjXr02ScOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ascii_image= image_to_ascii_art(image_rgb, output_path = 'ascii_lena.png')\n",
        "\n",
        "print(ascii_image)\n",
        "# Visualizamos la imagen\n",
        "plt.imshow(ascii_image)\n",
        "plt.title(\"Lenna\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HOaKOWTmTVh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Procesar video\n",
        "\n",
        "Una vez visto como se procesan imágenes, podemos plantearnos el procesado de vídeos. En este caso la forma que tendremos de hacerlo con OpenCV es ir recorriendo el video frame a frame, y procesar cada uno de ellos de forma independiente.\n",
        "\n",
        "Escoged el video que querais para esta tarea. Lo más rápido puede ser cogerlo de Youtube. Coged un video corto, tipo *short* para que los tiempos de procesamiento sean menores. Para descargar el vídeo utilizamos la librería `yt-dlp`.\n"
      ],
      "metadata": {
        "id": "97nXcgb-06ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalamos yt-dlp\n",
        "!pip install yt-dlp"
      ],
      "metadata": {
        "id": "WAe6909T1Vii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yt_dlp\n",
        "\n",
        "video_url = \"https://www.youtube.com/shorts/n96w_-EXieY\"\n",
        "output_path = \"short_video.mp4\"\n",
        "\n",
        "# Configurar opciones para descargar el mejor mp4 posible\n",
        "ydl_opts = {\n",
        "    'outtmpl': output_path,   # nombre del archivo de salida\n",
        "    'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',  # preferir mp4\n",
        "    'quiet': True\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])\n",
        "\n",
        "print(\"Video descargado:\", output_path)\n"
      ],
      "metadata": {
        "id": "HiZDTvGJxMA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez descargado ya podemos empezar a trabajar en él. Para ello nos valemos de la clase `VideoCapture()` para crear el objeto video. De esta podemos extraer multitud de parámetros del vídeo. Como queremos guardar el video una vez procesado recogemos los fps y tamaño del video original. El objeto sobre el que guardamos el vídeo es de la clase `VideoWriter()`. En ambos objetos, al finalizalir el procesamiento deberemos aplicar su método *release()*.\n",
        "\n",
        "> La clase `VideoCapture()` es realmente útil para aplicaciones en tiempo real, ya que permite recibir distintas entradas de vídeo, como cámaras conectadas al equipo, o *streaming*.\n",
        "\n",
        "Vamos a obtener los bordes de la imagen usando los filtros de Sobel:"
      ],
      "metadata": {
        "id": "8OdFWBhx25Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"short_video.mp4\"\n",
        "output_path = \"video_sobel.avi\"\n",
        "\n",
        "# Abrir el video original\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "# Obtener dimensiones y FPS del video de entrada\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Crear VideoWriter para el resultado (video en escala de grises)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height), False)\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convertir a escala de grises\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Aplicar Sobel en X e Y\n",
        "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "\n",
        "    # Calcular magnitud del gradiente y normalizar\n",
        "    sobel_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
        "    sobel_magnitude = np.uint8(255 * sobel_magnitude / np.max(sobel_magnitude))\n",
        "\n",
        "    # Escribir el frame procesado directamente en el nuevo video\n",
        "    out.write(sobel_magnitude)\n",
        "\n",
        "    frame_idx += 1\n",
        "    if frame_idx % 50 == 0:\n",
        "        print(f\"Procesados {frame_idx} frames...\")\n",
        "\n",
        "# Liberar recursos\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"Video procesado guardado en: {output_path}\")"
      ],
      "metadata": {
        "id": "kaQni2Je1rxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y al igual que hicimos con la imagen, también podríamos quantizar el color del vídeo."
      ],
      "metadata": {
        "id": "Wyy3yTAnaze2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_video = quick_quantize_video('short_video.mp4', output_path='video_quant.mp4', colors = 5)"
      ],
      "metadata": {
        "id": "azAaF59za5kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas son algunas de las muchas cosas que se pueden plantear con la librería OpenCV. Si queréis formaros más en este tema, disponéis de unos [tutoriales](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) muy buenos en su página principal."
      ],
      "metadata": {
        "id": "5czI2YgjuyGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Procesamiento con YOLO**\n",
        "\n",
        "\n",
        " <img src=\"https://cdn.prod.website-files.com/680a070c3b99253410dd3dcf/68e4eb1e9893320b26cc02c3_Ultralytics%20Logo.png.svg\" alt=\"Logo de OpenCV\" width=\"300\" style=\"margin-right: 400px;\">"
      ],
      "metadata": {
        "id": "bzXoAJBe_VAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección vamos a explorar una aplicación más avanzada de la visión artificial:\n",
        "la detección y/o reconocimiento de objetos mediante redes neuronales convolucionales.\n",
        "\n",
        "Para ello utilizaremos la librería `ultralytics`, que incluye los modelos YOLO\n",
        "(*You Only Look Once*). Estos modelos destacan por su rapidez y precisión,\n",
        "siendo ampliamente usados en detección de personas, vehículos o animales en tiempo real.\n",
        "\n"
      ],
      "metadata": {
        "id": "gsjJUzaPF05g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalamos la librería con el siguiente comando:"
      ],
      "metadata": {
        "id": "GIz6q1dAGF3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalamos ultralytics\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "m9OdL4le4VrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics"
      ],
      "metadata": {
        "id": "Nhh9iio53-L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos ahora un modelo preentrenado de YOLO.\n",
        "Si no se indica un modelo específico, por defecto se utiliza `yolov8s.pt`. Pero es una versión un tanto desactualizada.\n",
        "\n",
        "Por ello, en este caso usaremos `yolo11s.pt`, una versión optimizada y ligera, adecuada para demostraciones y ejecución rápida en CPU."
      ],
      "metadata": {
        "id": "EeXnI4lbGLg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos la detección con el modelo cargado. El método `model()` realiza todo el proceso:\n",
        "- Inferencia sobre la imagen.\n",
        "- Detección de objetos y sus coordenadas.\n",
        "- Asignación de etiquetas y confianza."
      ],
      "metadata": {
        "id": "8wKkGAnGGYqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# Cargar un modelo pre-entrenado (por ejemplo, YOLOv8s)\n",
        "# Se usará yolov8s.pt por defecto si no se especifica otro\n",
        "model = YOLO('yolo11s.pt')\n",
        "\n",
        "# URL de la imagen para detección\n",
        "image_url = \"https://ultralytics.com/images/bus.jpg\"\n",
        "\n",
        "# Descargamos la imagen desde la URL dada\n",
        "image_rgb = get_img_from_url(url=image_url)\n",
        "\n",
        "\n",
        "# Ejecutar la detección en la imagen\n",
        "results = model(image_rgb)\n",
        "\n",
        "# Guardar la imagen con las detecciones en el directorio por defecto ('runs/detect/...')\n",
        "for r in results:\n",
        "    im_array = r.plot()  # Ploteo de imagen RGB en formato numpy de las predicciones del modelo ejecutado\n",
        "    im = cv2.cvtColor(im_array, cv2.COLOR_RGB2BGR) # Convertimos RGB a BGR\n",
        "    output_path = 'output_yolo.png'  # Elegimos un nombre para guardar la imagen\n",
        "    cv2.imwrite(output_path, im) # Finalmente guardamos la imagen BGR con OpenCV,\n",
        "\n",
        "\n",
        "print(f\"Imagen con detecciones guardada en: {output_path}\")\n",
        "\n",
        "img_to_plot = np.concatenate((image_rgb, im_array), axis=1)\n",
        "# Opcional: mostrar la imagen con detecciones (puede ser grande)\n",
        "plt.imshow(img_to_plot)\n",
        "# plt.title(\"YOLO Detecciones\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b_nHSBr94Fyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos aplicar exactamente el mismo proceso a un vídeo. La librería `ultralytics`\n",
        "permite procesar directamente vídeos o flujos de cámara mediante el método `predict()`.\n",
        "\n",
        "El parámetro `save=True` guarda automáticamente el vídeo anotado en el directorio `runs/detect/`."
      ],
      "metadata": {
        "id": "ZhsDlqO3GkQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Cargar un modelo pre-entrenado (por ejemplo, YOLOv8s)\n",
        "model = YOLO('yolo11s.pt')\n",
        "\n",
        "# Ruta del video descargado\n",
        "video_path = 'short_video.mp4'\n",
        "\n",
        "# Ejecutar la detección en el video\n",
        "# El método predict de YOLO puede procesar videos directamente\n",
        "results = model.predict(video_path, stream=True, save=True, verbose=False) # save=True guarda el video con las detecciones\n",
        "\n",
        "print(\"Procesamiento del video con YOLO completado. El video con las detecciones se ha guardado en el directorio 'runs/detect'.\")"
      ],
      "metadata": {
        "id": "TMxS5PJR3OoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya para terminar, vamos a darle un uso a lo que hemos visto en el ejemplo anterior. Aprovechando la detección de personas, vamos a darle una aplicación muy sencilla y a la vez muy util.\n",
        "\n",
        "En concreto, vamos a usar YOLO para contar personas dentro de una región de un vídeo dentro de una cola de embarque en un aeropuerto.\n",
        "Este tipo de solución se usa en control de aforos o sistemas automáticos en aeropuertos y estaciones.\n",
        "\n",
        "Para ello utilizaremos el módulo `ObjectCounter` incluido en `ultralytics.solutions`."
      ],
      "metadata": {
        "id": "EbbKwr-_Gqaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics.utils.downloads import safe_download\n",
        "from ultralytics import solutions\n",
        "\n",
        "# Descarga de un video de ejemplo\n",
        "safe_download(\"https://github.com/ultralytics/notebooks/releases/download/v0.0.0/queue-management-demo.mp4\")\n",
        "\n",
        "\n",
        "def count_objects_in_region(video, output_video_path, model_path, classes= [0]):\n",
        "    \"\"\"Cuenta objetos dentro de una región específica del vídeo.\"\"\"\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    assert cap.isOpened(), \"Error leyendo el archivo de video\"\n",
        "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "    # Región de interés definida por cuatro puntos\n",
        "    region_points = [(250, 570), (1200, 350), (1200, 390), (250, 610)]\n",
        "\n",
        "    # Inicializamos el contador de objetos\n",
        "    counter = solutions.ObjectCounter(show=True, show_out=False, verbose=False, region=region_points, model=model_path, classes = classes)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, im0 = cap.read()\n",
        "        if not success:\n",
        "            print(\"Video finalizado o frame vacío.\")\n",
        "            break\n",
        "        results = counter(im0)\n",
        "        video_writer.write(results.plot_im)\n",
        "\n",
        "    cap.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Ejecutamos la función de conteo sobre el video de ejemplo\n",
        "count_objects_in_region(\"queue-management-demo.mp4\", \"output_video_queue.avi\", \"yolo11n.pt\")"
      ],
      "metadata": {
        "id": "XJ-R6-hejRIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}